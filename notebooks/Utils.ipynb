{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e383b6-c1cd-493a-a9a7-04a29a44e6c4",
   "metadata": {},
   "source": [
    "# Utils notebook\n",
    "\n",
    "This python notebook has util function for data analysis\n",
    "\n",
    "1) To generate the overview picture of dataset\n",
    "2) To generte diffs files for all patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da74ffa",
   "metadata": {},
   "source": [
    "### Following is the code to generate all_patches_stats later saved at results/smartbugs/data_analysis/all_patches_stats.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc105ec5-c31d-47d2-a25f-157b611e6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    'Elysium', \n",
    "    'sGuard', \n",
    "    'sGuardPlus',\n",
    "    'SmartFix', \n",
    "    'SmartShield', \n",
    "    \"SolGPT\" ,\n",
    "    'TIPS', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee218795-9e15-4d37-ad8d-1e2d9846d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "Columns=['Patch','Original','Category','Tool','DIFF','COMP', 'Detected','Fixed','Consistent','functional_check','mitigates','patch_link', 'original_link', 'diff_link', 'exploit_link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac64f74f-8df3-40fc-a6a3-241eb6acc7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#collecting contracts name\n",
    "dataset='smartbugs/data_analysis/'\n",
    "results_path='../results/'\n",
    "contracts_path=os.path.join(results_path,dataset,'contracts.csv')\n",
    "storage_path=os.path.join(results_path,dataset)\n",
    "\n",
    "import pandas as pd\n",
    "contracts_df = pd.read_csv(contracts_path)\n",
    "result_df = pd.DataFrame([], columns=Columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69c1500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vulnerabilities=contracts_df[\"Category\"].unique().tolist()\n",
    "contract_names=contracts_df[\"Name\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd09f77a-9efc-40a8-8d8f-ca7d2365c947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patch</th>\n",
       "      <th>Original</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tool</th>\n",
       "      <th>DIFF</th>\n",
       "      <th>COMP</th>\n",
       "      <th>Detected</th>\n",
       "      <th>Fixed</th>\n",
       "      <th>Consistent</th>\n",
       "      <th>functional_check</th>\n",
       "      <th>mitigates</th>\n",
       "      <th>patch_link</th>\n",
       "      <th>original_link</th>\n",
       "      <th>diff_link</th>\n",
       "      <th>exploit_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Patch, Original, Category, Tool, DIFF, COMP, Detected, Fixed, Consistent, functional_check, mitigates, patch_link, original_link, diff_link, exploit_link]\n",
       "Index: []"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Verify the columns\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6588786c-b5e7-4987-bb38-5c7c837bbc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_contract(input_string):\n",
    "    parts = input_string.split('/')\n",
    "    return parts[-2]  \n",
    "\n",
    "def get_category(text):\n",
    "    folders = os.path.normpath(text).split(os.sep)\n",
    "    \n",
    "    # Check each folder against the predefined category list\n",
    "    for folder in folders:\n",
    "        if folder in vulnerabilities:\n",
    "            return folder\n",
    "    return \"unknown\" \n",
    "def get_patch(input_string):\n",
    "    parts = input_string.split('/')\n",
    "    return parts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fa3f8ad-412d-4c1b-a0db-7a215cb75991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "\n",
    "def test_n_patches(result_df):\n",
    "    assert len(result_df[result_df['Tool'] == 'Elysium'].values.tolist()) == 121\n",
    "    assert len(result_df[result_df['Tool'] == 'SmartShield'].values.tolist()) == 135\n",
    "    assert len(result_df[result_df['Tool'] == 'sGuard'].values.tolist()) == 109\n",
    "    assert len(result_df[result_df['Tool'] == 'sGuardPlus'].values.tolist()) == 81\n",
    "    assert len(result_df[result_df['Tool'] == 'SmartFix'].values.tolist()) == 86\n",
    "    assert len(result_df[result_df['Tool'] == 'TIPS'].values.tolist()) == 242\n",
    "    assert len(result_df[result_df['Tool'] == 'SolGPT'].values.tolist()) == 552\n",
    "def test_dataset(result_df):\n",
    "    assert len(result_df['Original'].unique()) == 141\n",
    "    assert len(result_df['Category'].unique()) ==10\n",
    "    assert len(result_df['Tool'].unique()) == 7\n",
    "\n",
    "def test_diff_patches(result_df):\n",
    "    assert len(result_df[(result_df['Tool'] == 'Elysium') & (result_df['DIFF'] == True)]) == 95\n",
    "    assert len(result_df[(result_df['Tool'] == 'SmartShield') & (result_df['DIFF'] == True)]) == 135\n",
    "    assert len(result_df[(result_df['Tool'] == 'sGuard') & (result_df['DIFF'] == True)]) == 62\n",
    "    assert len(result_df[(result_df['Tool'] == 'sGuardPlus') & (result_df['DIFF'] == True)]) == 81\n",
    "    assert len(result_df[(result_df['Tool'] == 'SmartFix') & (result_df['DIFF'] == True)]) == 86\n",
    "    assert len(result_df[(result_df['Tool'] == 'TIPS') & (result_df['DIFF'] == True)]) == 242\n",
    "    assert len(result_df[(result_df['Tool'] == 'SolGPT') & (result_df['DIFF'] == True)]) == 552\n",
    "def test_compilable_patches(result_df):\n",
    "    assert len(result_df[(result_df['Tool'] == 'Elysium') & (result_df['COMP'] == True)]) == 0  # Elysium does not have compilable data\n",
    "    assert len(result_df[(result_df['Tool'] == 'SmartShield') & (result_df['COMP'] == True)]) == 0  # Smartshield does not have compilable data\n",
    "    assert len(result_df[(result_df['Tool'] == 'sGuard') & (result_df['COMP'] == True)]) == 108\n",
    "    assert len(result_df[(result_df['Tool'] == 'sGuardPlus') & (result_df['COMP'] == True)]) == 81\n",
    "    assert len(result_df[(result_df['Tool'] == 'SmartFix') & (result_df['COMP'] == True)]) == 86\n",
    "    assert len(result_df[(result_df['Tool'] == 'TIPS') & (result_df['COMP'] == True)]) == 234\n",
    "    assert len(result_df[(result_df['Tool'] == 'SolGPT') & (result_df['COMP'] == True)]) == 527\n",
    "\n",
    "def test_valid_patches(result_df):\n",
    "    assert len(result_df[(result_df['Tool'] == 'Elysium') & (result_df['Valid Patches'] == True)]) == 101  # Elysium does not have compilable patches\n",
    "    assert len(result_df[(result_df['Tool'] == 'SmartShield') & (result_df['Valid Patches'] == True)]) == 131  # Smartshield does not have compilable patches\n",
    "    assert len(result_df[(result_df['Tool'] == 'sGuard') & (result_df['Valid Patches'] == True)]) == 61\n",
    "    assert len(result_df[(result_df['Tool'] == 'sGuardPlus') & (result_df['Valid Patches'] == True)]) == 81\n",
    "    assert len(result_df[(result_df['Tool'] == 'SmartFix') & (result_df['Valid Patches'] == True)]) == 86\n",
    "    assert len(result_df[(result_df['Tool'] == 'TIPS') & (result_df['Valid Patches'] == True)]) == 234\n",
    "    assert len(result_df[(result_df['Tool'] == 'SolGPT') & (result_df['Valid Patches'] == True)]) == 527\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c44f19b6-db25-449a-9b82-eece37fc75ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_diff(tools, result_df):\n",
    "    for tool in tools:\n",
    "        tool_diff_file='../results/smartbugs/'+tool+'/patches_diff.csv'\n",
    "        df_diff=pd.read_csv(tool_diff_file, names=['Patch', 'Original','DIFF'], skiprows=1)\n",
    "        if tool in ['SolGPT']:\n",
    "            df_diff=pd.read_csv(tool_diff_file, names=['Patch', 'Original','cleaned','DIFF'], skiprows=1)\n",
    "        values=df_diff.values.tolist()\n",
    "        for line in values:\n",
    "            category= get_category(line[0])\n",
    "            patch = get_patch(line[0])\n",
    "            original= get_contract(line[0])+ '.sol'\n",
    "            diff= line[-1]\n",
    "            patch_link= 'https://github.com/ASSERT-KTH/RepairComp/blob/main/results/smartbugs/'+tool+'/'+line[0]\n",
    "            original_link='https://github.com/smartbugs/smartbugs-curated/tree/main/dataset/'+category+'/'+original\n",
    "            diff_link= 'https://github.com/ASSERT-KTH/RepairComp/blob/main/results/smartbugs/'+tool+'/'+line[0] .replace('.sol', '.diff')\n",
    "            if tool in ['Elysium', 'SmartShield']:\n",
    "                patch=patch[:-7]+'.bin'\n",
    "                diff_link=\"n/a\"\n",
    "            new_row = {'Patch': patch, 'Category': category, 'Original': original, 'Tool':tool, 'DIFF': diff,'patch_link':patch_link, 'original_link':original_link, 'diff_link': diff_link}\n",
    "            result_df = pd.concat([result_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    return result_df\n",
    "                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59a84ab5-2521-468b-a17d-a7f469db84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_comp(tools,result_df):\n",
    "    for tool in tools:\n",
    "        tool_comp_file='../results/smartbugs/'+tool+'/compilation_results_0.4.24.csv'\n",
    "        df_comp=pd.read_csv(tool_comp_file, names=['Patch', 'COMP'], skiprows=1)\n",
    "        values=df_comp.values.tolist()\n",
    "        for line in values:\n",
    "            category= get_category(line[0])\n",
    "            patch = get_patch(line[0])\n",
    "            original= get_contract(line[0])+ '.sol'\n",
    "            comp= line[-1]\n",
    "            if comp == 0:\n",
    "                comp = True\n",
    "            else :\n",
    "                comp = False\n",
    "            mask = (result_df['Patch'] == patch) & (result_df['Category'] == category) & (result_df['Tool'] == tool)\n",
    "            if mask.any():\n",
    "                result_df.loc[mask, 'COMP'] = comp\n",
    "    return result_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "696463cd-7ae2-4ce3-a97a-09a064672f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_d_f_m(tools,result_df):\n",
    "    for tool in tools:\n",
    "        tool_dfm_file='../results/smartbugs/'+tool+'/patches_evaluation.csv'\n",
    "        df_dfm=pd.read_csv(tool_dfm_file, names=['Patch', 'Original','Detected', 'Fixed','Maintained'], skiprows=1)\n",
    "        values=df_dfm.values.tolist()\n",
    "        for line in values:\n",
    "            category= get_category(line[1])\n",
    "            patch = get_patch(line[0])\n",
    "            original= get_contract(line[0])+ '.sol'\n",
    "            D= line[-3]\n",
    "            F=line[-2]\n",
    "            mask = (result_df['Patch'] == patch) & (result_df['Category'] == category) & (result_df['Tool'] == tool)\n",
    "            result_df.loc[mask, 'Detected'] = D\n",
    "            result_df.loc[mask, 'Fixed']= F\n",
    "            result_df.loc[mask, 'Consistent'] = True if D == F and D>0 else False\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01639eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 91\n",
      "\n",
      "Files per category:\n",
      "category\n",
      "reentrancy                   26\n",
      "unchecked_low_level_calls    20\n",
      "access_control               16\n",
      "arithmetic                   13\n",
      "bad_randomness                4\n",
      "denial_of_service             4\n",
      "front_running                 3\n",
      "time_manipulation             3\n",
      "other                         2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#collecting exploits\n",
    "#from curl to https://github.com/ASSERT-KTH/sb-heists/blob/main/smartbugs-curated/0.4.x/contracts_w_exploits.csv\n",
    "import requests\n",
    "import json\n",
    "from io import StringIO\n",
    "\n",
    "# Get the raw GitHub content\n",
    "url = \"https://raw.githubusercontent.com/ASSERT-KTH/sb-heists/main/smartbugs-curated/0.4.x/contracts_w_exploits.csv\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Read the CSV content from the response\n",
    "    csv_content = StringIO(response.text)\n",
    "    \n",
    "    # Load the file paths\n",
    "    exploits = pd.read_csv(csv_content, header=None, names=['file_path'])\n",
    "    \n",
    "    # Split the file_path into category and name\n",
    "    exploits[['category', 'name']] = exploits['file_path'].str.split('/', n=1, expand=True)\n",
    "    \n",
    "    # Display the first few rows to verify\n",
    "    print(f\"Total files: {len(exploits)}\")\n",
    "    \n",
    "    # Get a count of files by category\n",
    "    print(\"\\nFiles per category:\")\n",
    "    print(exploits['category'].value_counts())\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "\n",
    "def mark_no_exploit_contracts(result_df, exploits_df):\n",
    "    \"\"\"\n",
    "    Mark contracts with no associated exploit by setting functional_check, mitigates and exploit_link to 'NONE'.\n",
    "    \n",
    "    Parameters:\n",
    "    result_df (DataFrame): DataFrame with contracts to be analyzed\n",
    "    exploits_df (DataFrame): DataFrame containing information about available exploits\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Updated DataFrame with NONE values for contracts without exploits\n",
    "    \"\"\"\n",
    "    # Create a set of (category, name) tuples for contracts with exploits\n",
    "    exploit_pairs = set(zip(exploits_df['category'], exploits_df['name']))\n",
    "    \n",
    "    # Create a mask for rows that don't have an exploit\n",
    "    no_exploit_mask = ~result_df.apply(lambda row: (row['Category'], row['Original']) in exploit_pairs, axis=1)\n",
    "    \n",
    "    # Set 'NONE' values for contracts without exploits\n",
    "    result_df.loc[no_exploit_mask, 'functional_check'] = 'non-exploitable'\n",
    "    result_df.loc[no_exploit_mask, 'mitigates'] = 'non-exploitable'\n",
    "    result_df.loc[no_exploit_mask, 'exploit_link'] = 'non-exploitable'\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Total contracts without exploits: {no_exploit_mask.sum()}\")\n",
    "    print(f\"Total contracts with exploits: {len(result_df) - no_exploit_mask.sum()}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def read_functional_and_exploits(tools, result_df):\n",
    "\n",
    "    \"\"\"\n",
    "    Read functional and exploit test results for each tool and update the functional_check and mitigates columns.\n",
    "    \n",
    "    Parameters:\n",
    "    tools (list): List of tool names to process\n",
    "    result_df (DataFrame): DataFrame with columns including Category, Tool, functional_check, mitigates, etc.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Updated DataFrame with functional_check and mitigates columns filled based on test results\n",
    "    \"\"\"\n",
    "\n",
    "    result_df = mark_no_exploit_contracts(result_df, exploits)\n",
    "    \n",
    "    for tool in tools:\n",
    "        file_path = f'../results/smartbugs/{tool}/results_exploits.json'\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"Processing results for tool: {tool}\")\n",
    "            with open(file_path, 'r') as file:\n",
    "                result_data = json.load(file)\n",
    "                \n",
    "                # Create a mapping to store test results by contract file\n",
    "                contract_results = {}\n",
    "                \n",
    "                # Process failed functional tests\n",
    "                for test in result_data.get('failedSanityTests', []):\n",
    "                    contract_file = test.get('contractFile')\n",
    "                    test_file= test.get('file')\n",
    "\n",
    "                    if contract_file:\n",
    "                        contract_results[contract_file] = {'functional': 'failed', 'exploit': None, 'test_file': test_file}\n",
    "                \n",
    "                # Process passed exploit tests\n",
    "                for test in result_data.get('passedResults', []):\n",
    "                    contract_file = test.get('contractFile')\n",
    "                    test_file= test.get('file')\n",
    "                    if contract_file:\n",
    "                        if contract_file not in contract_results:\n",
    "                            contract_results[contract_file] = {'functional': 'passed', 'exploit': 'passed', 'test_file': test_file}\n",
    "                        else:\n",
    "                            contract_results[contract_file]['exploit'] = 'passed'\n",
    "                \n",
    "                # Process failed exploit tests\n",
    "                for test in result_data.get('failedResults', []):\n",
    "                    contract_file = test.get('contractFile')\n",
    "                    test_file= test.get('file')\n",
    "                    if contract_file:\n",
    "                        if contract_file not in contract_results:\n",
    "                            contract_results[contract_file] = {'functional': 'passed', 'exploit': 'failed', 'test_file': test_file}\n",
    "                        else:\n",
    "                            contract_results[contract_file]['exploit'] = 'failed'\n",
    "                \n",
    "                # Update rows where Tool matches the current tool\n",
    "                tool_mask = result_df['Tool'] == tool\n",
    "                \n",
    "                # Only update rows that don't already have 'non-exploitable' values (skip contracts with no exploits)\n",
    "                update_mask = tool_mask & (result_df['functional_check'] != 'non-exploitable')\n",
    "                \n",
    "                # Update the DataFrame using vectorized operations where possible\n",
    "                for idx, row in result_df[update_mask].iterrows():\n",
    "                    # Construct contract path to match with what's in the test results\n",
    "                    if tool in ['Elysium', 'SmartShield', 'sGuardPlus']:\n",
    "                        contract_path = f\"{row['Category']}/{row['Original']}\"\n",
    "                    else:\n",
    "                        contract_path = f\"{row['Category']}/{row['Patch']}\"\n",
    "                    result = contract_results.get(contract_path, {})\n",
    "\n",
    "\n",
    "                    \n",
    "                    # Update mitigates based on functional and exploit results\n",
    "                    functional = result.get('functional', 'unknown')\n",
    "                    exploit = result.get('exploit', 'unknown')\n",
    "                    \n",
    "                    # Update functional_check\n",
    "                    if functional == 'passed' or functional == 'failed':\n",
    "                         result_df.at[idx, 'functional_check'] = functional\n",
    "                    \n",
    "                    # Update mitigates based on functional and exploit results\n",
    "                    functional = result.get('functional', 'unknown')\n",
    "                    exploit = result.get('exploit', 'unknown')\n",
    "                    \n",
    "                    if functional == 'passed' and exploit == 'failed':\n",
    "                        result_df.at[idx, 'mitigates'] = 'yes'\n",
    "                    elif exploit == 'passed':\n",
    "                        result_df.at[idx, 'mitigates'] = 'no'\n",
    "                    test_file= result.get('test_file', None)\n",
    "                    # Update exploit link\n",
    "                    exploit_link = f\"https://github.com/ASSERT-KTH/sb-heists/tree/main/smartbugs-curated/0.4.x/test/{test_file}\"\n",
    "                    result_df.at[idx, 'exploit_link'] = exploit_link\n",
    "\n",
    "\n",
    "                \n",
    "                # Print summary for this tool\n",
    "                print(f\"\\nSummary for {tool}:\")\n",
    "                tool_rows = result_df[result_df['Tool'] == tool]\n",
    "                functional_counts = tool_rows['functional_check'].value_counts().to_dict()\n",
    "                mitigates_counts = tool_rows['mitigates'].value_counts().to_dict()\n",
    "                print(f\"functional check counts: {functional_counts}\")\n",
    "                print(f\"Mitigates counts: {mitigates_counts}\")\n",
    "        else:\n",
    "            print(f\"Results file not found for tool: {tool}\")\n",
    "    \n",
    "    return result_df\n",
    "                \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df042ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c6cbaa5-5976-4594-a48a-37de6c810035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total contracts without exploits: 493\n",
      "Total contracts with exploits: 833\n",
      "Processing results for tool: Elysium\n",
      "\n",
      "Summary for Elysium:\n",
      "functional check counts: {'passed': 48, 'non-exploitable': 47, 'failed': 26}\n",
      "Mitigates counts: {'non-exploitable': 47, 'yes': 29, 'no': 20}\n",
      "Processing results for tool: sGuard\n",
      "\n",
      "Summary for sGuard:\n",
      "functional check counts: {'passed': 45, 'non-exploitable': 41, 'failed': 1}\n",
      "Mitigates counts: {'non-exploitable': 41, 'yes': 30, 'no': 15}\n",
      "Processing results for tool: sGuardPlus\n",
      "\n",
      "Summary for sGuardPlus:\n",
      "functional check counts: {'passed': 49, 'non-exploitable': 32}\n",
      "Mitigates counts: {'yes': 44, 'non-exploitable': 32, 'no': 5}\n",
      "Processing results for tool: SmartFix\n",
      "\n",
      "Summary for SmartFix:\n",
      "functional check counts: {'passed': 60, 'non-exploitable': 22, 'failed': 4}\n",
      "Mitigates counts: {'yes': 48, 'non-exploitable': 22, 'no': 12}\n",
      "Processing results for tool: SmartShield\n",
      "\n",
      "Summary for SmartShield:\n",
      "functional check counts: {'passed': 67, 'non-exploitable': 47, 'failed': 21}\n",
      "Mitigates counts: {'non-exploitable': 47, 'no': 41, 'yes': 26}\n",
      "Processing results for tool: SolGPT\n",
      "\n",
      "Summary for SolGPT:\n",
      "functional check counts: {'passed': 298, 'non-exploitable': 204, 'failed': 27}\n",
      "Mitigates counts: {'yes': 247, 'non-exploitable': 204, 'no': 52}\n",
      "Processing results for tool: TIPS\n",
      "\n",
      "Summary for TIPS:\n",
      "functional check counts: {'passed': 135, 'non-exploitable': 100, 'failed': 5}\n",
      "Mitigates counts: {'non-exploitable': 100, 'no': 78, 'yes': 57}\n"
     ]
    }
   ],
   "source": [
    "result_df=read_diff(tools, result_df)\n",
    "#result_df['Detected'].fillna(None, inplace=True)\n",
    "#result_df['Fixed'].fillna(None, inplace=True)\n",
    "\n",
    "comp_tools=[\n",
    "    #'Elysium', \n",
    "    'sGuard', \n",
    "    'sGuardPlus',\n",
    "    'SmartFix', \n",
    "    #'SmartShield', \n",
    "    \"SolGPT\" ,\n",
    "    'TIPS', \n",
    "]\n",
    "non_comp_tools=[\n",
    "    'Elysium', \n",
    "    'SmartShield'\n",
    "]\n",
    "result_df=read_comp(comp_tools, result_df)\n",
    "result_df['COMP'].fillna(\"N/A\", inplace=True)\n",
    "\n",
    "result_df=read_d_f_m(tools,result_df)\n",
    "\n",
    "result_df=read_functional_and_exploits(tools, result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5342c17d",
   "metadata": {},
   "source": [
    "#### A few functional checks to ensure data correctly captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "abaadb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values, if good nothing should be printed\n",
    "for tool in tools:\n",
    "    query= result_df[(result_df['Tool']==tool)& (result_df['Fixed']>0)]\n",
    "    if len(query.values.tolist()) == 0:\n",
    "        print(tool)\n",
    "        print(query)\n",
    "        print('----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b2e4248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elysium\n",
      "sGuard\n",
      "sGuardPlus\n",
      "SmartFix\n",
      "SmartShield\n",
      "SolGPT\n",
      "TIPS\n"
     ]
    }
   ],
   "source": [
    "# all tools should have Fixed >0 for at least one patch\n",
    "for tool in tools:\n",
    "    print(tool)\n",
    "    assert(len(result_df[(result_df['Tool']==tool)& (result_df['Fixed']>0)].values.tolist()))>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a30b871b-a205-45f7-a835-14762bfee0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity of results checks\n",
    "test_n_patches(result_df)\n",
    "test_dataset(result_df)\n",
    "test_diff_patches(result_df)\n",
    "test_compilable_patches(result_df)\n",
    "#test_valid_patches(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7dc36a04-4575-4bdf-bc03-a7cd03f91f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn to csv\n",
    "#[TODO] Uncomment to save the results\n",
    "#result_df.to_csv(os.path.join(storage_path,'all_patches_stats.csv'), index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67a7fca",
   "metadata": {},
   "source": [
    "### 2) Code to generate diff files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f67e9b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def read_file(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.readlines()\n",
    "\n",
    "def diff_files(file1, file2, file1_path, file2_path):\n",
    "    formatted1 = read_file(file1)\n",
    "    formatted2 = read_file(file2)\n",
    "\n",
    "    diff = difflib.unified_diff(\n",
    "        formatted1, formatted2,\n",
    "        fromfile=file1_path, tofile=file2_path,\n",
    "        lineterm=''\n",
    "    )\n",
    "\n",
    "    return '\\n'.join(diff)\n",
    "\n",
    "def patch_path(path):\n",
    "    github_prefix = \"https://github.com/ASSERT-KTH/RepairComp/blob/main/results/\"\n",
    "\n",
    "    dir_prefix = \"../results/\"\n",
    "\n",
    "    path = path.replace(github_prefix, dir_prefix)\n",
    "    return path\n",
    "\n",
    "def og_path(path):\n",
    "    github_prefix = \"https://github.com/smartbugs/smartbugs-curated/tree/main/\"\n",
    "    dir_prefix = \"../../smartbugs-curated/\"\n",
    "    path = path.replace(github_prefix, dir_prefix)\n",
    "    return path\n",
    "\n",
    "def generate_diffs():\n",
    "    for vuln in vulnerabilities:\n",
    "        # read the patches\n",
    "        patches_vuln = pd.read_csv(os.path.join(storage_path, vuln + '_patches.csv'))\n",
    "\n",
    "        # iterate patch_link and original_link from the patches using zip\n",
    "        for patch_link, original_link in zip(patches_vuln['patch_link'], patches_vuln['original_link']):\n",
    "            # patch_path and og_path are functions that replace the prefix of the path\n",
    "            patch = patch_path(patch_link)\n",
    "            original = og_path(original_link)\n",
    "\n",
    "            #  check if patch and original exist\n",
    "            if not os.path.exists(patch):\n",
    "                print(f\"Patch file does not exist: {patch}\")\n",
    "                # continue\n",
    "            if not os.path.exists(original):\n",
    "                print(f\"Original file does not exist: {original}\")\n",
    "                # continue\n",
    "\n",
    "            # diff_files is a function that returns the diff between two files\n",
    "            diff = diff_files(original, patch, original_link, patch_link)\n",
    "\n",
    "            filename = patch.replace('.sol', '.diff')\n",
    "            print(\"Filename: \", filename)\n",
    "\n",
    "            # Save the diff to a file\n",
    "            with open(filename, 'w') as f:\n",
    "                f.write(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf92802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_path = os.path.join(storage_path, 'all_patches_stats.csv')\n",
    "def generate_diffs_for_all(force_regenerate=True):\n",
    "    \"\"\"\n",
    "    Generate diff files for all source code patches in all_patches_stats.csv \n",
    "    Places diff files in the same directory as each patch file.\n",
    "    Uses the existing patch_path, og_path, and diff_files functions.\n",
    "    \n",
    "    Args:\n",
    "        force_regenerate (bool): If True, regenerate even if diff file exists\n",
    "    \"\"\"\n",
    "    all_patches_stats = pd.read_csv(patches_path)\n",
    "\n",
    "\n",
    "    print(f\"Processing {len(all_patches_stats)} patches for diff generation...\")\n",
    "    if force_regenerate:\n",
    "        print(\"⚠️  Force regeneration enabled - will replace existing diff files\")\n",
    "    \n",
    "    stats = {\n",
    "        'total': len(all_patches_stats),\n",
    "        'generated': 0,\n",
    "        'replaced': 0,\n",
    "        'skipped_existing': 0,\n",
    "        'skipped_missing_files': 0,\n",
    "        'errors': 0\n",
    "    }\n",
    "    \n",
    "    # Iterate through all patches in all_patches_stats\n",
    "    for idx, row in all_patches_stats.iterrows():\n",
    "        patch_link = row['patch_link']\n",
    "        original_link = row['original_link']\n",
    "        \n",
    "        try:\n",
    "            # Convert GitHub URLs to local paths using existing functions\n",
    "            patch = patch_path(patch_link)\n",
    "            original = og_path(original_link)\n",
    "            \n",
    "            # Skip bytecode files (.bin, .hex) since we don't have source code to compare\n",
    "            patch_basename = os.path.basename(patch)\n",
    "            patch_ext = os.path.splitext(patch_basename)[1].lower()\n",
    "            \n",
    "            if patch_ext in ['.bin', '.hex']:\n",
    "                print(f\"Skipping bytecode file: {patch_basename}\")\n",
    "                stats['skipped_missing_files'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Create diff filename in the same directory as the patch file\n",
    "            patch_dir = os.path.dirname(patch)\n",
    "            \n",
    "            # Remove extension and add .diff\n",
    "            diff_basename = os.path.splitext(patch_basename)[0] + '.diff'\n",
    "            diff_filename = os.path.join(patch_dir, diff_basename)\n",
    "            \n",
    "            # Check if diff already exists\n",
    "            if os.path.exists(diff_filename):\n",
    "                if force_regenerate:\n",
    "                    print(f\"Replacing existing diff: {diff_basename}\")\n",
    "                    stats['replaced'] += 1\n",
    "                else:\n",
    "                    print(f\"Diff already exists, skipping: {diff_filename}\")\n",
    "                    stats['skipped_existing'] += 1\n",
    "                    continue\n",
    "            \n",
    "            # Check if source files exist\n",
    "            missing_files = []\n",
    "            if not os.path.exists(patch):\n",
    "                missing_files.append(f\"Patch: {patch}\")\n",
    "            if not os.path.exists(original):\n",
    "                missing_files.append(f\"Original: {original}\")\n",
    "            \n",
    "            if missing_files:\n",
    "                print(f\"Missing files for row {idx}:\")\n",
    "                for missing in missing_files:\n",
    "                    print(f\"  - {missing}\")\n",
    "                stats['skipped_missing_files'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Generate the diff using existing function (fix parameter order)\n",
    "            print(f\"Generating diff for: {patch_basename} -> {diff_basename}\")\n",
    "            diff_content = diff_files(original, patch, original_link, patch_link)\n",
    "            \n",
    "            # Ensure patch directory exists (should already exist if patch file exists)\n",
    "            if not os.path.exists(patch_dir):\n",
    "                os.makedirs(patch_dir, exist_ok=True)\n",
    "            \n",
    "            # Save the diff to file in the same directory as the patch\n",
    "            with open(diff_filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(diff_content)\n",
    "            \n",
    "            if not os.path.exists(diff_filename) or stats['replaced'] == 0:\n",
    "                stats['generated'] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            print(f\"  Patch link: {patch_link}\")\n",
    "            print(f\"  Original link: {original_link}\")\n",
    "            stats['errors'] += 1\n",
    "            continue\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n=== Diff Generation Summary ===\")\n",
    "    print(f\"Total patches processed: {stats['total']}\")\n",
    "    print(f\"New diffs generated: {stats['generated']}\")\n",
    "    print(f\"Existing diffs replaced: {stats['replaced']}\")\n",
    "    print(f\"Already existing (skipped): {stats['skipped_existing']}\")\n",
    "    print(f\"Missing source files (skipped): {stats['skipped_missing_files']}\")\n",
    "    print(f\"Errors encountered: {stats['errors']}\")\n",
    "    \n",
    "    total_created = stats['generated'] + stats['replaced']\n",
    "    if total_created > 0:\n",
    "        print(f\"\\n✅ Successfully created/replaced {total_created} diff files!\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  No diff files were generated or replaced.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47d3173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all diffs, replacing existing ones (recommended for your situation)\n",
    "# If you want to avoid replacing existing diffs, set force_regenerate=False\n",
    "# [TODO] Uncomment the line below to run the diff generation\n",
    "#generate_diffs_for_all(force_regenerate=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
